{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simpliest usage example of py_boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation (if needed)\n",
    "\n",
    "**Note**: replace cupy-cuda110 with your cuda version!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cupy-cuda110 py-boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Optional: set the device to run\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# simple case - just one class is used\n",
    "from py_boost import GradientBoosting, CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of dummy regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 1.37 s, total: 3.66 s\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = make_regression(150000, 100, n_targets=10, random_state=42)\n",
    "X_test, y_test = X[:50000], y[:50000]\n",
    "X, y = X[-50000:], y[-50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_boost.gpu.utils import *\n",
    "from py_boost.gpu.tree import *\n",
    "from py_boost.gpu.base import Ensemble\n",
    "\n",
    "def cluster_grow_tree(tree, group, arr, grad, hess, row_indexer, col_indexer, params):\n",
    "    # create gh\n",
    "    n_out = grad.shape[1]\n",
    "    gh = cp.concatenate((grad, hess), axis=1)\n",
    "    out_indexer = cp.arange(gh.shape[1], dtype=cp.uint64)\n",
    "\n",
    "    # init nodes with single zero node\n",
    "    unique_nodes = np.zeros(1, dtype=np.int32)\n",
    "    # count unique nodes in active rows\n",
    "    nodes_count = cp.ones(1, dtype=cp.uint64) * row_indexer.shape[0]\n",
    "    # nodes for all rows\n",
    "    nodes = cp.zeros(arr.shape[0], dtype=cp.int32)\n",
    "    # index of node in unique array\n",
    "    node_indexes = nodes\n",
    "    prev_hist, small_index, big_index = [None] * 3\n",
    "\n",
    "    for niter in range(params['max_depth']):\n",
    "\n",
    "        nnodes = len(unique_nodes)\n",
    "        gh_hist = histogram(arr, gh, node_indexes,\n",
    "                            col_indexer=col_indexer,\n",
    "                            row_indexer=row_indexer,\n",
    "                            out_indexer=out_indexer,\n",
    "                            nnodes=nnodes,\n",
    "                            max_bins=params['max_bin'],\n",
    "                            prev_hist=prev_hist,\n",
    "                            small_index=small_index,\n",
    "                            big_index=big_index)\n",
    "\n",
    "        # assume hess is the last output\n",
    "\n",
    "        hist, counts = gh_hist[:-1], gh_hist[-1]\n",
    "        total = hist[..., :1, -1:]\n",
    "        curr = total.min(axis=0)\n",
    "        gain = cp.zeros(hist.shape[1:] + (2,), dtype=cp.float32)\n",
    "        \n",
    "        # NAN to left\n",
    "        gain[..., 0] = curr - hist.min(axis=0) - (total - hist).min(axis=0)\n",
    "        gain[..., 0] *= cp.minimum(counts, counts[..., -1:] - counts) >= params['min_data_in_leaf']\n",
    "        \n",
    "        # NAN to right\n",
    "        gain[..., 1] = curr - (hist - hist[..., :1]).min(axis=0) - (total - hist + hist[..., :1]).min(axis=0)\n",
    "        gain[..., 1] *= cp.minimum(counts - counts[..., :1:], counts[..., -1:] - counts + counts[..., :1]) >= params['min_data_in_leaf']\n",
    "\n",
    "        best_feat, best_gain, best_split, best_nan_left = get_best_split(gain, col_indexer)\n",
    "\n",
    "        # move to CPU and apply min_gain_to_split condition\n",
    "        unique_nodes, new_nodes_id, best_feat, best_gain, best_split, best_nan_left, is_valid_node = \\\n",
    "            get_cpu_splitters(unique_nodes, best_feat, best_gain, best_split, best_nan_left,\n",
    "                              params['min_gain_to_split'])\n",
    "        # if all nodes are not valid to split - exit\n",
    "        if len(unique_nodes) == 0:\n",
    "            break\n",
    "        # write node info to the Tree\n",
    "        tree.set_nodes(group, unique_nodes, new_nodes_id, best_feat, best_gain, best_split, best_nan_left)\n",
    "        # get args back on gpu\n",
    "        split_args, unique_nodes = get_gpu_splitters(unique_nodes, new_nodes_id,\n",
    "                                                     best_feat, best_split, best_nan_left)\n",
    "\n",
    "        # perform split for train set\n",
    "        nodes, node_indexes = make_split(nodes, arr, *split_args, return_pos=True)\n",
    "\n",
    "        # update info for the next step\n",
    "        if niter < (params['max_depth'] - 1):\n",
    "            # update counts\n",
    "            nodes_count = cp.zeros((unique_nodes.shape[0] + 1,), dtype=np.uint64)\n",
    "            nodes_count.scatter_add(node_indexes[row_indexer], 1)\n",
    "            nodes_count = nodes_count[:-1]\n",
    "\n",
    "            cpu_counts = nodes_count.get()\n",
    "\n",
    "            # remove unused rows from indexer\n",
    "            if cpu_counts.sum() < row_indexer.shape[0]:\n",
    "                row_indexer = row_indexer[isin(nodes, split_args[1].ravel(), index=row_indexer)]\n",
    "\n",
    "            # save histogram for the subs trick\n",
    "            prev_hist, small_index, big_index = get_prev_hist(cpu_counts,\n",
    "                                                              gh_hist, cp.asarray(is_valid_node))\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "\n",
    "class ClusterTreeBuilder:\n",
    "    \"\"\"Tree builder for early stopping clusters\"\"\"\n",
    "\n",
    "    def __init__(self, borders,\n",
    "                 **tree_params\n",
    "                 ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            borders: list of np.ndarray, actual split borders for quantized features\n",
    "            **tree_params: other tree building parameters\n",
    "        \"\"\"\n",
    "        self.borders = borders\n",
    "\n",
    "        self.params = {**{\n",
    "\n",
    "            'max_bin': 256,\n",
    "            'max_depth': 6,\n",
    "            'min_data_in_leaf': 10,\n",
    "            'min_gain_to_split': 0\n",
    "\n",
    "        }, **tree_params}\n",
    "\n",
    "\n",
    "    def build_tree(self, X, y):\n",
    "        \"\"\"Build tree\n",
    "\n",
    "        Args:\n",
    "            X: cp.ndarray, quantized feature matrix\n",
    "            y: cp.ndarray, loss path matrix\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            tree, Tree, constructed tree\n",
    "        \"\"\"\n",
    "\n",
    "        col_indexer = cp.arange(X.shape[1], dtype=cp.uint64)\n",
    "        row_indexer = cp.arange(X.shape[0], dtype=cp.uint64)\n",
    "        max_nodes = int((2 ** np.arange(self.params['max_depth'] + 1)).sum())\n",
    "        tree = Tree(max_nodes, y.shape[1], 1)\n",
    "        # grow single group of the tree and get nodes index\n",
    "        cluster_grow_tree(tree, 0, X, y, cp.ones((y.shape[0], 1), dtype=cp.float32),\n",
    "                          row_indexer, col_indexer, self.params)\n",
    "\n",
    "        tree.set_borders(self.borders)\n",
    "        tree.set_leaves()\n",
    "        tree.set_node_values(np.zeros((max_nodes, 1) , dtype=np.float32), np.zeros((1, ) , dtype=np.uint64))\n",
    "\n",
    "        return tree\n",
    "    \n",
    "class ClusterCandidates(Ensemble):\n",
    "    \n",
    "    def __init__(self, depth_range=list(range(1, 7)), min_data_in_leaf=100, max_bin=256, quant_sample=100000):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.depth_range = depth_range\n",
    "        self.min_data_in_leaf = min_data_in_leaf\n",
    "        self.max_clust = 2 ** max(depth_range)\n",
    "        self.max_bin = max_bin\n",
    "        self.quant_sample = quant_sample\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X, y, sample_weight, eval_sets = validate_input(X, y, None, [])\n",
    "        mempool = cp.cuda.MemoryPool()\n",
    "        with cp.cuda.using_allocator(allocator=mempool.malloc):\n",
    "            X_enc, max_bin, borders, eval_enc = quantize_train_valid(X, eval_sets, self.max_bin, self.quant_sample)\n",
    "            \n",
    "            self.fit_quantized(X_enc, y, max_bin, borders)\n",
    "        mempool.free_all_blocks()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def fit_quantized(self, X_enc, y, max_bin, borders):\n",
    "        \n",
    "        y = cp.array(y, order='C', dtype=cp.float32)\n",
    "        X_cp = pad_and_move(X_enc)\n",
    "        self.models = []\n",
    "        \n",
    "        for d in self.depth_range:\n",
    "            builder = ClusterTreeBuilder(borders, max_depth=d, min_data_in_leaf=500, max_bin=max_bin)\n",
    "            self.models.append(builder.build_tree(X_cp, y))\n",
    "            \n",
    "        self.base_score = np.zeros((1, ), dtype=np.float32)\n",
    "            \n",
    "        return self\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = np.abs(y)\n",
    "\n",
    "# X, y, sample_weight, eval_sets = validate_input(X, y)\n",
    "# X_enc, max_bin, borders, eval_enc = quantize_train_valid(X, eval_sets, 255, 10000)\n",
    "# X_cp = pad_and_move(X_enc)\n",
    "# y_ = cp.array(y_, order='C', dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 16 ms, total: 1.47 s\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ClusterCandidates at 0x7feb05d550d0>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clusters = ClusterCandidates(min_data_in_leaf=10)\n",
    "clusters.fit(X, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91, -1, -1]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.models[0].feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4471349, 0.       , 0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.models[0].val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  7, 12, 14, 16], dtype=uint32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.predict_leaves(X)[..., 0].T.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = tree.predict_leaf(cp.array(X, dtype=cp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(17, dtype=uint32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17], dtype=uint32),\n",
       " array([  731,   612,   815,   522,   699,   598,   912,   691,   513,\n",
       "          541,   605,   528,   608,   506,   836,   742, 37867,  1674]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.unique(leaves, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a GBDT model\n",
    "\n",
    "The only argument required here is a loss function. It, together with the input target shape, determines the task type. The loss function can be passed as a Loss instance or using a string alias:\n",
    "\n",
    "* ***'mse'*** for the regression/multitask regression\n",
    "* ***'msle'*** for the regression/multitask regression\n",
    "* ***'bce'*** for the binary/multilabel classification\n",
    "* ***'crossentropy'*** for the multiclassification\n",
    "\n",
    "Training is simply done by calling the .fit metod. Possible argumentsare the following:\n",
    "\n",
    "* ***'X'*** \n",
    "* ***'y'*** \n",
    "* ***'sample_weight'*** \n",
    "* ***'eval_sets'***  \n",
    "A validation set is passed as a list of dicts with possible keys ['X', 'y', 'sample_weight']. Note: if multiple valid sets are passed, the best model is selected using the last one.\n",
    "\n",
    "#### The example below illustrates how to train a simple regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:46:02] Stdout logging level is INFO.\n",
      "[07:46:02] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:03] Iter 0; Sample 0, rmse = 173.67502218528736; \n",
      "[07:46:03] Iter 10; Sample 0, rmse = 133.1954913545181; \n",
      "[07:46:03] Iter 20; Sample 0, rmse = 107.86651497941278; \n",
      "[07:46:03] Iter 30; Sample 0, rmse = 90.08231837066428; \n",
      "[07:46:03] Iter 40; Sample 0, rmse = 76.44512114655686; \n",
      "[07:46:03] Iter 50; Sample 0, rmse = 65.61043014181283; \n",
      "[07:46:03] Iter 60; Sample 0, rmse = 56.801627968047306; \n",
      "[07:46:03] Iter 70; Sample 0, rmse = 49.57726379316838; \n",
      "[07:46:03] Iter 80; Sample 0, rmse = 43.603736430179794; \n",
      "[07:46:03] Iter 90; Sample 0, rmse = 38.69769943824404; \n",
      "[07:46:03] Iter 99; Sample 0, rmse = 34.99192512700217; \n",
      "CPU times: user 4.28 s, sys: 740 ms, total: 5.02 s\n",
      "Wall time: 3.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.boosting.GradientBoosting at 0x7f4d8c780eb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = GradientBoosting('mse')\n",
    "\n",
    "model.fit(X, y[:, 0], eval_sets=[{'X': X_test, 'y': y_test[:, 0]},])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:46:04] Stdout logging level is INFO.\n",
      "[07:46:04] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:04] Iter 0; Sample 0, rmse = 172.9149002108474; \n",
      "[07:46:04] Iter 10; Sample 0, rmse = 132.55210391589216; \n",
      "[07:46:04] Iter 20; Sample 0, rmse = 107.23296689918294; \n",
      "[07:46:04] Iter 30; Sample 0, rmse = 89.55113466574534; \n",
      "[07:46:04] Iter 40; Sample 0, rmse = 76.07330856675229; \n",
      "[07:46:04] Iter 50; Sample 0, rmse = 65.31944162376402; \n",
      "[07:46:04] Iter 60; Sample 0, rmse = 56.69942793984432; \n",
      "[07:46:04] Iter 70; Sample 0, rmse = 49.5628290696496; \n",
      "[07:46:04] Iter 80; Sample 0, rmse = 43.681215651498924; \n",
      "[07:46:04] Iter 90; Sample 0, rmse = 38.90670685220184; \n",
      "[07:46:04] Iter 99; Sample 0, rmse = 35.28539972019422; \n",
      "[07:46:05] Stdout logging level is INFO.\n",
      "[07:46:05] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:05] Iter 0; Sample 0, rmse = 177.05438336288094; \n",
      "[07:46:05] Iter 10; Sample 0, rmse = 135.3641459583856; \n",
      "[07:46:05] Iter 20; Sample 0, rmse = 109.47371792928402; \n",
      "[07:46:05] Iter 30; Sample 0, rmse = 91.3265814134245; \n",
      "[07:46:05] Iter 40; Sample 0, rmse = 77.67765460386947; \n",
      "[07:46:05] Iter 50; Sample 0, rmse = 66.68704751302562; \n",
      "[07:46:05] Iter 60; Sample 0, rmse = 57.91666878757441; \n",
      "[07:46:05] Iter 70; Sample 0, rmse = 50.67433643152712; \n",
      "[07:46:05] Iter 80; Sample 0, rmse = 44.76402567685365; \n",
      "[07:46:05] Iter 90; Sample 0, rmse = 39.888733962494776; \n",
      "[07:46:05] Iter 99; Sample 0, rmse = 36.275957742357825; \n",
      "[07:46:05] Stdout logging level is INFO.\n",
      "[07:46:05] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:05] Iter 0; Sample 0, rmse = 176.65031878069445; \n",
      "[07:46:06] Iter 10; Sample 0, rmse = 135.44628458647136; \n",
      "[07:46:06] Iter 20; Sample 0, rmse = 109.70817632395016; \n",
      "[07:46:06] Iter 30; Sample 0, rmse = 91.74843818900366; \n",
      "[07:46:06] Iter 40; Sample 0, rmse = 78.0511355098756; \n",
      "[07:46:06] Iter 50; Sample 0, rmse = 67.12045255132357; \n",
      "[07:46:06] Iter 60; Sample 0, rmse = 58.3326633220076; \n",
      "[07:46:06] Iter 70; Sample 0, rmse = 51.12294061481514; \n",
      "[07:46:06] Iter 80; Sample 0, rmse = 45.17325918208381; \n",
      "[07:46:06] Iter 90; Sample 0, rmse = 40.177450510130114; \n",
      "[07:46:06] Iter 99; Sample 0, rmse = 36.509147507296774; \n",
      "[07:46:06] Stdout logging level is INFO.\n",
      "[07:46:06] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:06] Iter 0; Sample 0, rmse = 174.62949610948496; \n",
      "[07:46:06] Iter 10; Sample 0, rmse = 133.68721418794814; \n",
      "[07:46:07] Iter 20; Sample 0, rmse = 107.99985521543748; \n",
      "[07:46:07] Iter 30; Sample 0, rmse = 90.16107504388509; \n",
      "[07:46:07] Iter 40; Sample 0, rmse = 76.46287633799443; \n",
      "[07:46:07] Iter 50; Sample 0, rmse = 65.63076145907517; \n",
      "[07:46:07] Iter 60; Sample 0, rmse = 56.87751547683644; \n",
      "[07:46:07] Iter 70; Sample 0, rmse = 49.7364996954236; \n",
      "[07:46:07] Iter 80; Sample 0, rmse = 43.92484077983016; \n",
      "[07:46:07] Iter 90; Sample 0, rmse = 39.08841055490191; \n",
      "[07:46:07] Iter 99; Sample 0, rmse = 35.46163733230166; \n",
      "[07:46:08] Stdout logging level is INFO.\n",
      "[07:46:08] GDBT train starts. Max iter 100, early stopping rounds 100\n",
      "[07:46:08] Iter 0; Sample 0, rmse = 175.49684554342025; \n",
      "[07:46:08] Iter 10; Sample 0, rmse = 134.43641615372448; \n",
      "[07:46:08] Iter 20; Sample 0, rmse = 108.77365656298713; \n",
      "[07:46:08] Iter 30; Sample 0, rmse = 90.69728144249562; \n",
      "[07:46:08] Iter 40; Sample 0, rmse = 76.8761160361599; \n",
      "[07:46:08] Iter 50; Sample 0, rmse = 66.01371477464895; \n",
      "[07:46:08] Iter 60; Sample 0, rmse = 57.19904173706199; \n",
      "[07:46:08] Iter 70; Sample 0, rmse = 49.97159375101946; \n",
      "[07:46:08] Iter 80; Sample 0, rmse = 44.14922317954969; \n",
      "[07:46:08] Iter 90; Sample 0, rmse = 39.28913355770473; \n",
      "[07:46:08] Iter 99; Sample 0, rmse = 35.63765995993683; \n",
      "CPU times: user 6.22 s, sys: 147 ms, total: 6.37 s\n",
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = GradientBoosting('mse')\n",
    "cv = CrossValidation(model)\n",
    "\n",
    "oof = cv.fit_predict(X, y[:, 0], stratify=False)\n",
    "test_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traininig a GBDT model in a multiregression case\n",
    "\n",
    "Each of built-in loss functions has its own default metric, so metric definition is optional. \n",
    "If you need to specify the evaluation metric, you can pass a Metric instance or use a string alias.\n",
    "\n",
    "#### Default metrics:\n",
    "\n",
    "* ***'rmse'*** is the default for the ***'mse'*** loss\n",
    "* ***'rmsle'*** is the default for the  ***'msle'*** loss\n",
    "* ***'bce'*** is the default for the ***'bce'*** loss\n",
    "* ***'crossentropy'*** is the default for the ***'crossentropy'*** loss\n",
    "\n",
    "#### Non-default metrics:\n",
    "\n",
    "* ***'r2'*** for the regression/multitask regression\n",
    "* ***'auc'*** for the binary classification\n",
    "* ***'accuracy'*** for any classification task\n",
    "* ***'precision'*** for any classification task\n",
    "* ***'recall'*** for any classification task\n",
    "* ***'f1'*** for any classification task\n",
    "\n",
    "It is possible to specify other common GBDT hyperparameters as shown below.\n",
    "\n",
    "#### The following example demonstrates how to train a model for a multioutput regression task (no extra definition needed to switch the task to multioutput one, you just need to pass a multidimensional target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:46:36] Stdout logging level is INFO.\n",
      "[07:46:36] GDBT train starts. Max iter 1000, early stopping rounds 200\n",
      "[07:46:37] Iter 0; Sample 0, R2_score = 0.008384933833777941; \n",
      "[07:46:39] Iter 100; Sample 0, R2_score = 0.5168161670121248; \n",
      "[07:46:41] Iter 200; Sample 0, R2_score = 0.7243374908930145; \n",
      "[07:46:43] Iter 300; Sample 0, R2_score = 0.8327434907624806; \n",
      "[07:46:45] Iter 400; Sample 0, R2_score = 0.894976532253023; \n",
      "[07:46:47] Iter 500; Sample 0, R2_score = 0.9320361103269559; \n",
      "[07:46:49] Iter 600; Sample 0, R2_score = 0.9546979928604653; \n",
      "[07:46:51] Iter 700; Sample 0, R2_score = 0.9687510714926756; \n",
      "[07:46:54] Iter 800; Sample 0, R2_score = 0.9776249822188479; \n",
      "[07:46:56] Iter 900; Sample 0, R2_score = 0.9833214913041353; \n",
      "[07:46:58] Iter 999; Sample 0, R2_score = 0.9870160617953221; \n",
      "CPU times: user 20.4 s, sys: 2.74 s, total: 23.2 s\n",
      "Wall time: 21.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.boosting.GradientBoosting at 0x7f4ae971a0a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = GradientBoosting('mse', 'r2_score',\n",
    "                         ntrees=1000, lr=.01, verbose=100, es=200, lambda_l2=1,\n",
    "                         subsample=.8, colsample=.8, min_data_in_leaf=10, min_gain_to_split=0, \n",
    "                         max_bin=256, max_depth=6)\n",
    "\n",
    "model.fit(X, y, eval_sets=[{'X': X_test, 'y': y_test},])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_boost.gpu.losses import BCELoss, BCEMetric\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "class BCENanMetric(BCEMetric):\n",
    "    \n",
    "    alias = 'score'\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        \n",
    "        mask = ~cp.isnan(y_true)\n",
    "        y_true = cp.where(mask, y_true, 0)\n",
    "        err = self.error(y_true, y_pred)\n",
    "        \n",
    "        shape = err.shape\n",
    "        assert shape[0] == y_true.shape[0], 'Error shape should match target shape at first dim'\n",
    "\n",
    "        if len(shape) == 1:\n",
    "            err = err[:, cp.newaxis]\n",
    "\n",
    "        if sample_weight is None:\n",
    "            return (err * mask).sum() / mask.sum()\n",
    "        \n",
    "        sw = sample_weight[mask]\n",
    "        \n",
    "        err = ((err * mask).sum(axis=1, keepdims=True) / mask.sum(axis=1, keepdims=True) * sw).sum() / sw.sum()\n",
    "        return err\n",
    "        \n",
    "\n",
    "class BCENanLoss(BCELoss):\n",
    "    \n",
    "    def base_score(self, y_true):\n",
    "        means = cp.clip(cp.nanmean(y_true, axis=0), self.clip_value, 1 - self.clip_value)\n",
    "        return cp.log(means / (1 - means))\n",
    "    \n",
    "    def get_grad_hess(self, y_true, y_pred):\n",
    "        \n",
    "        mask = ~cp.isnan(y_true)\n",
    "        grad, hess = super().get_grad_hess(cp.where(mask, y_true, 0), y_pred)\n",
    "        grad = grad * mask\n",
    "        hess = hess * mask\n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "\n",
    "#model = GradientBoosting(BCENanLoss(), BCENanMetric()....)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = np.random.rand(*y.shape) > 0.5\n",
    "y_nan = np.where(sl, y > y.mean(), np.nan)\n",
    "\n",
    "sl = np.random.rand(*y_test.shape) > 0.5\n",
    "y_test_nan = np.where(sl, y_test > y_test.mean(), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:22:29] Stdout logging level is INFO.\n",
      "[10:22:29] GDBT train starts. Max iter 1000, early stopping rounds 200\n",
      "[10:22:29] Iter 0; Sample 0, score = 0.6901909016192641; \n",
      "[10:22:31] Iter 100; Sample 0, score = 0.49825635116672295; \n",
      "[10:22:33] Iter 200; Sample 0, score = 0.4038025931142952; \n",
      "[10:22:35] Iter 300; Sample 0, score = 0.3450437285302818; \n",
      "[10:22:37] Iter 400; Sample 0, score = 0.30451169662313754; \n",
      "[10:22:39] Iter 500; Sample 0, score = 0.2747490970224845; \n",
      "[10:22:41] Iter 600; Sample 0, score = 0.25168115713944844; \n",
      "[10:22:43] Iter 700; Sample 0, score = 0.23333456700279837; \n",
      "[10:22:45] Iter 800; Sample 0, score = 0.2182130401657076; \n",
      "[10:22:47] Iter 900; Sample 0, score = 0.20563296666317543; \n",
      "[10:22:49] Iter 999; Sample 0, score = 0.19491068344038848; \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_boost.gpu.boosting.GradientBoosting at 0x7f4ae7efc190>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoosting(BCENanLoss(), BCENanMetric(),\n",
    "                         ntrees=1000, lr=.01, verbose=100, es=200, lambda_l2=1,\n",
    "                         subsample=.8, colsample=.8, min_data_in_leaf=10, min_gain_to_split=0, \n",
    "                         max_bin=256, max_depth=6)\n",
    "\n",
    "model.fit(X, y_nan, eval_sets=[{'X': X_test, 'y': y_test_nan},])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50206699, 0.5017048 , 0.49735283, 0.50005686, 0.50266285,\n",
       "       0.50213899, 0.50314733, 0.50159522, 0.49943213, 0.5041649 ])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(X_test) * (~np.isnan(y_test_nan))).sum(axis=0) / (~np.isnan(y_test_nan)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-c07c3d144bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/cupy/_sorting/search.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_fusing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_where_ufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_where_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "cp.where(~np.isnan(y_test_nan), y_test_nan, 0).sum(axis=0) / (~np.isnan(y_test_nan)).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993794985643834"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test_nan[:, 0][~np.isnan(y_test_nan[:, 0])], model.predict(X_test)[:, 0][~np.isnan(y_test_nan[:, 0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "#### Prediction can be done via calling the .predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 940 ms, sys: 757 ms, total: 1.7 s\n",
      "Wall time: 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-241.72522  , -147.04788  , -279.85645  , ..., -141.56766  ,\n",
       "        -213.81375  , -235.54959  ],\n",
       "       [-110.30279  , -112.996796 ,  -60.784744 , ..., -128.7514   ,\n",
       "        -119.637596 ,  -21.854822 ],\n",
       "       [ -32.4901   ,  -55.91898  ,  145.11751  , ...,   19.574871 ,\n",
       "         -20.344044 , -206.56976  ],\n",
       "       ...,\n",
       "       [ -75.05906  ,  134.71219  ,   78.33846  , ...,  224.71863  ,\n",
       "          38.28656  ,   16.880033 ],\n",
       "       [  -4.0309997,  143.80292  ,  250.64313  , ...,  154.68509  ,\n",
       "         180.606    ,  212.36513  ],\n",
       "       [ -20.445889 ,   34.293472 ,  168.47072  , ...,   94.26082  ,\n",
       "          21.152784 ,    3.533686 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for certan iterations can be done via calling the .predict_staged method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 312 ms, sys: 382 ms, total: 694 ms\n",
      "Wall time: 739 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 50000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict_staged(X_test, iterations=[100, 300, 500])\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree leaves indicies prediction for certan iterations can be done via calling the .predict_leaves method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 3.96 ms, total: 19.8 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 50000, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = model.predict_leaves(X_test, iterations=[100, 300, 500])\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 21,  7],\n",
       "       [54, 46, 18],\n",
       "       [32, 46, 53],\n",
       "       ...,\n",
       "       [54, 53, 10],\n",
       "       [27, 46, 15],\n",
       "       [60, 46, 18]], dtype=uint32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  43.,   45.,   36.,   49.,   63.,   47., 5721.,   48.,   39.,\n",
       "         66.,   38.,   42.,   50.,   53.,   33., 5873., 5508.,   44.,\n",
       "         32., 5535.,   32.,   37.,   50.,   70.,   34.,   44.,   46.,\n",
       "         47.,   47.,   47.,   54.,   49.,   45.,   45.,   35.,   45.,\n",
       "       6070.,   31.,   47.,   38.,   49.,   49.,   51.,   42.,   54.,\n",
       "         67.,   50.,   58.,   42.,   51.,   50.,   61., 5875.,   36.,\n",
       "         58.,   62.,   44.,   21.,   27.,   35.,   47.,   39.,   45.,\n",
       "         44.,   48.,   45.,   46.,   37.,   54.,   55.,   54.,   38.,\n",
       "         38.,   48.,   34.,   54.,   60.,   37.,   36.,   55.,   61.,\n",
       "         40.,   47.,   35.,   63.,   26., 5635., 3557.,   49., 5737.,\n",
       "         47., 6077.,   38.,   45.,   43.,   45.,   35.,   45.,   35.,\n",
       "         57.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The trained model can be saved as pickle for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-241.72522  , -147.04788  , -279.85645  , ..., -141.56766  ,\n",
       "        -213.81375  , -235.54959  ],\n",
       "       [-110.30279  , -112.996796 ,  -60.784744 , ..., -128.7514   ,\n",
       "        -119.637596 ,  -21.854822 ],\n",
       "       [ -32.4901   ,  -55.91898  ,  145.11751  , ...,   19.574871 ,\n",
       "         -20.344044 , -206.56976  ],\n",
       "       ...,\n",
       "       [ -75.05906  ,  134.71219  ,   78.33846  , ...,  224.71863  ,\n",
       "          38.28656  ,   16.880033 ],\n",
       "       [  -4.0309997,  143.80292  ,  250.64313  , ...,  154.68509  ,\n",
       "         180.606    ,  212.36513  ],\n",
       "       [ -20.445889 ,   34.293472 ,  168.47072  , ...,   94.26082  ,\n",
       "          21.152784 ,    3.533686 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, '../data/temp_model.pkl')\n",
    "\n",
    "new_model = joblib.load('../data/temp_model.pkl')\n",
    "new_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda_py38",
   "language": "python",
   "name": "anaconda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
